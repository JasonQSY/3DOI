<html>
  <head>
    <title>Understanding 3D Object Interaction from a Single Image</title>
    <meta property="og:title" content="Understanding 3D Object Interaction from a Single Image"/>
    <meta property="og:image" content="teaser.png"/>
    <meta property="og:description" content="S. Qian, D. F. Fouhey." />
    <meta name="viewport" content="width=device-width, initial-scale=1">
    
    <!-- webpage template-->
    <link rel="stylesheet" href="website.css">  
  </head>
  <body>
    <br>
    <center>
      <span style="font-size:38px">Understanding 3D Object Interaction from a Single Image</span>
    </center>
    <br><br>
    <table align=center width=400px>
      <tr>
        <td align=center width=100px>
          <center>
            <span style="font-size:20px"><a href="https://jasonqsy.github.io/">Shengyi Qian</a></span>
          </center>
        </td>
        <td align=center width=100px>
          <center>
            <span style="font-size:20px"><a href="https://web.eecs.umich.edu/~fouhey/">David F. Fouhey</a></span>
          </center>
        </td>
      </tr>
    </table>
    <br>
    <table align=center width=700px>
      <tr>
        <td align=center width=200px>
          <center>
            <span style="font-size:20px">University of Michigan</span>
          </center>
        </td>
      </tr>
    </table>
    <br>
    <table align=center width=700px>
      <tr>
        <td align=center width=100px>
          <center>
            <span style="font-size:20px">ICCV 2023</span>
          </center>
        </td>
      </tr>
    </table>
    <br>
    <table align=center width=500px>
      <tr>
        <td align=center width=100px>
          <center>
            <span style="font-size:20px"><a href="https://arxiv.org/abs/2305.09664">[pdf]</a></span>
          </center>
        </td>
        <td align=center width=100px>
          <center>
            <span style="font-size:20px"><a href="https://github.com/JasonQSY/3DOI">[code]</a></span>
          </center>
        </td>
        <td align=center width=100px>
          <center>
            <span style="font-size:20px"><a href="https://www.youtube.com/watch?v=YDIL93XxHyk">[video]</a></span>
          </center>
        </td>
      </tr>
    </table>
    <br>
    <table align=center width=500px>
        <td align=center width=100px>
          <center>
            <span style="font-size:20px"><a href="https://openxlab.org.cn/apps/detail/JasonQSY/3DOI">[OpenXLab demo (gpu)]</a></span>
          </center>
        </td>
        <td align=center width=100px>
          <center>
            <span style="font-size:20px"><a href="https://huggingface.co/spaces/shengyi-qian/3DOI">[HuggingFace demo (cpu)]</a></span>
          </center>
        </td>
      </tr>
    </table>
    <table align=center width=700px>
      <tr>
        <td align=center width=100px>
          <center>
            <span style="font-size:20px"></span>
          </center>
        </td>
      </tr>
    </table>
    <br>
    <table align=center width=900px>
      <tr>
        <td width=00px>
          <center>
            <img src = "teaser.png" width="900px"></img><br>
          </center>
        </td>
      </tr>
      <!-- <td width=600px>
        <center>
          <span style="font-size:14px"><i>Given an ordinary video, our system produces a 3D planar representation of the observed articulation. The 3D renderings illustrate how the microwave (in Pink) can be articulated in 3D space. We also show the predicted rotation axis using a Blue arrow.</i>
        </center>
      </td>
      </tr> -->
    </table>
    <br>
    Humans can easily understand a single image as depicting multiple potential objects permitting interaction. We use this skill to plan our interactions with the world and accelerate understanding new objects without engaging in interaction. In this paper, we would like to endow machines with the similar ability, so that intelligent agents can better explore the 3D scene or manipulate objects. Our approach is a transformer-based model that predicts the 3D location, physical properties and affordance of objects. To power this model, we collect a dataset with Internet videos, egocentric videos and indoor images to train and validate our approach. Our model yields strong performance on our data, and generalizes well to robotics data.
    <br><br>
    <hr>
    <table align=center width=1100px>
      <tr>
        <td>
          <left>
            <center>
              <h1>3D Object Interaction Dataset</h1>
              <h3>Examples</h3>
            </center>
          </left>
        </td>
      </tr>
      <tr>
        <td>
          <left>
            <center>
              <img src = "dataset.png" width="900px"></img><br>
            </center>
          </left>
        </td>
      </tr>
    </table>
    <br>
    <table align=center width=1100px>
      <tr>
        <td>
          <left>
            <center>
              <h3>Downloads</h3>
            </center>
          </left>
        </td>
      </tr>
    </table>
    <table id="customers" align=center width=1000px>
      <tr>
          <th width=14%>Category</th>
          <th width=23%>Links</th>
          <th>Details</th>
      </tr>
      <tr>
          <td>Images</td>
          <td><a href="https://drive.google.com/file/d/1ZCUa9mHNCC1I2LOP-DqZEo2dEYHH8dao/view?usp=sharing">images.tar.gz</a></td>
          <td>All images from Articulation, Epickitchen and Omnidata. The usage of Epickitchen and Omnidata are subject to their original license (<a href="https://epic-kitchens.github.io/2023">Epickitchen</a> and <a href="https://github.com/EPFL-VILAB/omnidata/blob/main/LICENSE">Omnidata</a>).</td>
      </tr>
      <tr>
          <td>Annotations</td>
          <td><a href="https://drive.google.com/file/d/1ontxv4KDW7mtv4ErsDPC5MQVGK5M6Z4B/view?usp=share_link">3doi_v1.tar.gz</a></td>
          <td>Annotations for train, val and test split. The annotations of each split is stored in the pth file. Please check out our code about loading the data.</td>
      </tr>
      <tr>
          <td>Omnidata Ground Truth</td>
          <td><a href="https://drive.google.com/file/d/1c4Fi3bBf5slpBEio-0gMY09PcD0Yzqw_/view?usp=sharing">omnidata_filtered.tar.gz</a></td>
          <td>You can download full ground truth from Omnidata. However, we filter ground truth for images included in our data, and provide a separate download link.</td>
      </tr>
    </table>
    <br><br>
    <hr>
    <table align=center width=1100px>
      <tr>
        <td>
          <left>
            <center>
              <h1>Results</h1>
            </center>
          </left>
        </td>
      </tr>
    </table>
    <center>
      Interactive demo is available on <a href="https://huggingface.co/spaces/shengyi-qian/3DOI">Hugging Face</a>.
    </center>
    <br>
    <center>
      <img src = "results.png" width="900px"></img><br>
    </center>
    <br><br>
    <hr>
    <table align=center width=1100px>
      <tr>
        <td>
          <left>
            <center>
              <h1>Acknowledgements</h1>
            </center>
            This work was supported by the DARPA Machine Common Sense Program. This material is based upon work supported by the National Science Foundation under Grant No. 2142529. We also acknowledge the GPU support by OpenXLab for the demo.
          </left>
        </td>
      </tr>
    </table>
    <!-- Loads <model-viewer> for modern browsers: -->
    <script type="module"
      src="https://unpkg.com/@google/model-viewer@v0.9.0/dist/model-viewer.js"></script>
    <!-- Loads <model-viewer> for old browsers like IE11: -->
    <script nomodule
      src="https://unpkg.com/@google/model-viewer@v0.9.0/dist/model-viewer-legacy.js"></script>
  </body>
</html>
